{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cc299050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix, balanced_accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import random\n",
    "from typing import Tuple, List\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4356688a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4db527bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"Configuration class for model parameters\"\"\"\n",
    "    BASE_PATH = r\"D:\\Projects\\Cardiac Patient Monitoring System\\vitalldb\"\n",
    "    SIGNAL_NAMES = [\"HR\", \"SBP\", \"DBP\", \"MBP\", \"SpO2\", \"RR\"]\n",
    "    FILE_KEYWORDS = {\n",
    "        \"HR\": \"Solar8000_HR.csv\",\n",
    "        \"SBP\": \"Solar8000_ART_SBP.csv\",\n",
    "        \"DBP\": \"Solar8000_ART_DBP.csv\",\n",
    "        \"MBP\": \"Solar8000_ART_MBP.csv\",\n",
    "        \"SpO2\": \"Solar8000_PLETH_SPO2.csv\",\n",
    "        \"RR\": \"Solar8000_VENT_RR.csv\"\n",
    "    }\n",
    "    SEQUENCE_LEN = 60\n",
    "    BATCH_SIZE = 32 \n",
    "#     BATCH_SIZE = 16  # Reduced for memory efficiency\n",
    "    LEARNING_RATE = 0.001\n",
    "    NUM_EPOCHS = 15  # Reduced for faster training\n",
    "    HIDDEN_SIZE = 64  # Reduced for efficiency\n",
    "    DROPOUT_RATE = 0.3 #DROPOUT_RATE = 0.3<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "28473f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatientDataset(Dataset):\n",
    "    \"\"\"Custom dataset for patient sequences\"\"\"\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "72fab7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedLSTMClassifier(nn.Module):\n",
    "    \"\"\"Improved LSTM classifier with better architecture\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_classes=2, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Bidirectional LSTM for better pattern recognition\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=2,\n",
    "            dropout=dropout_rate,\n",
    "            batch_first=True,\n",
    "            bidirectional=True  # Better for capturing patterns\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism for better sequence understanding\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size * 2,  # *2 because bidirectional\n",
    "            num_heads=4,\n",
    "            dropout=dropout_rate,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_size * 2),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # LSTM forward pass\n",
    "        lstm_out, _ = self.lstm(x)  # [batch, seq_len, hidden_size * 2]\n",
    "        \n",
    "        # Self-attention to focus on important time steps\n",
    "        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
    "        \n",
    "        # Global average pooling instead of just last timestep\n",
    "        pooled = torch.mean(attn_out, dim=1)  # [batch, hidden_size * 2]\n",
    "        \n",
    "        # Classification\n",
    "        output = self.classifier(pooled)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bcbb312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FocalLoss(nn.Module):\n",
    "#     \"\"\"Focal Loss for handling class imbalance\"\"\"\n",
    "#     def __init__(self, alpha=None, gamma=2.0):\n",
    "#         super().__init__()\n",
    "#         self.alpha = alpha\n",
    "#         self.gamma = gamma\n",
    "        \n",
    "#     def forward(self, inputs, targets):\n",
    "#         ce_loss = nn.functional.cross_entropy(inputs, targets, reduction='none')\n",
    "#         pt = torch.exp(-ce_loss)\n",
    "#         focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "        \n",
    "#         if self.alpha is not None:\n",
    "#             alpha_t = self.alpha[targets]\n",
    "#             focal_loss = alpha_t * focal_loss\n",
    "            \n",
    "#         return focal_loss.mean()\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0, smoothing=0.05):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        num_classes = inputs.size(1)\n",
    "        true_dist = torch.zeros_like(inputs)\n",
    "        true_dist.fill_(self.smoothing / (num_classes - 1))\n",
    "        true_dist.scatter_(1, targets.unsqueeze(1), 1.0 - self.smoothing)\n",
    "\n",
    "        ce_loss = -(true_dist * torch.log_softmax(inputs, dim=1)).sum(dim=1)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha[targets]\n",
    "            focal_loss = alpha_t * focal_loss\n",
    "\n",
    "        return focal_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "37c3e0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(config: Config) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Load and preprocess patient data with improved memory management\"\"\"\n",
    "    \n",
    "    # Load clinical data\n",
    "    clinic_path = os.path.join(config.BASE_PATH, \"clinical_info.csv\")\n",
    "    clinic = pd.read_csv(clinic_path)\n",
    "    \n",
    "    # Create risk labels if not exists\n",
    "    if \"risk_label\" not in clinic.columns:\n",
    "        def assign_risk(row):\n",
    "            if row['death_inhosp'] == 1:\n",
    "                return 2  # Critical\n",
    "            elif row['icu_days'] >= 2 or row['asa'] >= 3:\n",
    "                return 1  # Moderate\n",
    "            else:\n",
    "                return 0  # Stable\n",
    "        clinic['risk_label'] = clinic.apply(assign_risk, axis=1)\n",
    "    \n",
    "    print(f\" Risk label distribution: {dict(clinic['risk_label'].value_counts())}\")\n",
    "    \n",
    "    # Data holders\n",
    "    all_sequences = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Process patients in batches to manage memory\n",
    "    for idx, case_row in enumerate(tqdm(clinic.itertuples(), desc=\"Processing patients\")):\n",
    "        caseid = f\"case{int(case_row.caseid):04d}\"\n",
    "        label = case_row.risk_label\n",
    "        \n",
    "        # Load all signal files for this patient\n",
    "        patient_dfs = []\n",
    "        skip_patient = False\n",
    "        \n",
    "        for signal, keyword in config.FILE_KEYWORDS.items():\n",
    "            file_path = os.path.join(config.BASE_PATH, f\"{caseid}_{keyword}\")\n",
    "            if os.path.exists(file_path):\n",
    "                try:\n",
    "                    df = pd.read_csv(file_path, names=[\"Time\", signal])\n",
    "                    df[\"Time\"] = pd.to_numeric(df[\"Time\"], errors='coerce')\n",
    "                    df = df.dropna(subset=[\"Time\"])\n",
    "                    if len(df) > 0:\n",
    "                        patient_dfs.append(df)\n",
    "                    else:\n",
    "                        skip_patient = True\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {file_path}: {e}\")\n",
    "                    skip_patient = True\n",
    "                    break\n",
    "            else:\n",
    "                skip_patient = True\n",
    "                break\n",
    "        \n",
    "        if skip_patient or len(patient_dfs) != len(config.SIGNAL_NAMES):\n",
    "            continue\n",
    "        \n",
    "        # Merge signals based on time\n",
    "        merged_df = patient_dfs[0]\n",
    "        for df in patient_dfs[1:]:\n",
    "            merged_df = pd.merge_asof(\n",
    "                merged_df.sort_values(\"Time\"),\n",
    "                df.sort_values(\"Time\"),\n",
    "                on=\"Time\",\n",
    "                direction='nearest'\n",
    "            )\n",
    "        \n",
    "        # Handle missing values\n",
    "        merged_df = merged_df.fillna(method='ffill').fillna(method='bfill')\n",
    "        merged_df = merged_df.drop(columns=[\"Time\"])\n",
    "        \n",
    "        # Skip if too short\n",
    "        if len(merged_df) < config.SEQUENCE_LEN + 1:\n",
    "            continue\n",
    "        \n",
    "        # Normalize data\n",
    "        scaler = StandardScaler()  # Better than MinMax for medical data\n",
    "        scaled_data = scaler.fit_transform(merged_df.values)\n",
    "        \n",
    "        # Create sequences with stride to reduce memory usage\n",
    "        stride = max(1, config.SEQUENCE_LEN // 4)  # Overlapping sequences\n",
    "        for i in range(0, len(scaled_data) - config.SEQUENCE_LEN, stride):\n",
    "            sequence = scaled_data[i:i + config.SEQUENCE_LEN]\n",
    "            all_sequences.append(sequence)\n",
    "            all_labels.append(label)\n",
    "    \n",
    "    print(f\" Total sequences created: {len(all_sequences)}\")\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(all_sequences, dtype=np.float32)\n",
    "    y = np.array(all_labels, dtype=np.int64)\n",
    "\n",
    "    y= np.where(y == 2, 1, 0)\n",
    "    return X, y,scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8dc4be1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_dataset(X: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Smart balancing strategy for imbalanced dataset\"\"\"\n",
    "    \n",
    "    print(\"Original class distribution:\", Counter(y))\n",
    "    \n",
    "    # Separate classes\n",
    "    indices_0 = np.where(y == 0)[0]\n",
    "    indices_1 = np.where(y == 1)[0]\n",
    "    indices_2 = np.where(y == 2)[0]\n",
    "    \n",
    "    # Strategic sampling to balance computational efficiency with performance\n",
    "    # More balanced approach - not too aggressive undersampling\n",
    "    n_class_0 = min(len(indices_0), 8000)  # Increased from 2000\n",
    "    n_class_1 = len(indices_1)  # Keep all class 1\n",
    "    n_class_2 = len(indices_2)  # Keep all class 2\n",
    "    \n",
    "    # Sample indices\n",
    "    sampled_0 = np.random.choice(indices_0, n_class_0, replace=False)\n",
    "    sampled_1 = indices_1\n",
    "    sampled_2 = indices_2\n",
    "    \n",
    "    # Combine samples\n",
    "    X_balanced = np.concatenate([X[sampled_0], X[sampled_1], X[sampled_2]])\n",
    "    y_balanced = np.concatenate([y[sampled_0], y[sampled_1], y[sampled_2]])\n",
    "    \n",
    "    # Data augmentation for minority classes\n",
    "    X_aug = []\n",
    "    y_aug = []\n",
    "    \n",
    "    # More aggressive augmentation for class 2 (critical cases)\n",
    "    augment_factors = {0: 0.0, 1: 2.0} # Augment class 1 by 30%, class 2 by 200%\n",
    "#     augment_factors = {1: 0.3, 2: 2.0} <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<befor edit\n",
    "#     for target_class in [1, 2]<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<befor edit\n",
    "    for target_class in [0 , 1]: # \n",
    "        class_indices = np.where(y_balanced == target_class)[0]\n",
    "        class_data = X_balanced[class_indices]\n",
    "        \n",
    "        # Calculate number of augmented samples needed\n",
    "        n_augment = int(len(class_data) * augment_factors[target_class])\n",
    "        \n",
    "        if n_augment > 0:\n",
    "            aug_indices = np.random.choice(len(class_data), n_augment, replace=True)\n",
    "            \n",
    "            for idx in aug_indices:\n",
    "                original_sample = class_data[idx]\n",
    "                \n",
    "                # Multiple augmentation techniques\n",
    "                augmented_samples = []\n",
    "                \n",
    "                # 1. Gaussian noise\n",
    "                noise = np.random.normal(0, 0.02, original_sample.shape)\n",
    "                augmented_samples.append(original_sample + noise)\n",
    "                \n",
    "                # 2. Scaling\n",
    "                scale_factor = np.random.uniform(0.95, 1.05)\n",
    "                augmented_samples.append(original_sample * scale_factor)\n",
    "                \n",
    "                # 3. Slight time shift (roll)\n",
    "                shift = np.random.randint(-2, 3)\n",
    "                augmented_samples.append(np.roll(original_sample, shift, axis=0))\n",
    "                \n",
    "                # FIX: Choose one augmentation randomly using integer index\n",
    "                chosen_idx = np.random.randint(0, len(augmented_samples))\n",
    "                chosen_augment = augmented_samples[chosen_idx]\n",
    "                X_aug.append(chosen_augment)\n",
    "                y_aug.append(target_class)\n",
    "    \n",
    "    # Combine original and augmented data\n",
    "    if X_aug:\n",
    "        X_aug = np.array(X_aug)\n",
    "        y_aug = np.array(y_aug)\n",
    "        X_final = np.concatenate([X_balanced, X_aug])\n",
    "        y_final = np.concatenate([y_balanced, y_aug])\n",
    "    else:\n",
    "        X_final = X_balanced\n",
    "        y_final = y_balanced\n",
    "    \n",
    "    print(\"Final class distribution:\", Counter(y_final))\n",
    "    return X_final, y_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "41fbeba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weighted_sampler(y_train: np.ndarray) -> WeightedRandomSampler:\n",
    "    \"\"\"Create weighted sampler for training\"\"\"\n",
    "    class_counts = Counter(y_train)\n",
    "    total_samples = len(y_train)\n",
    "    \n",
    "    # Calculate weights inversely proportional to class frequency\n",
    "    weights = {cls: total_samples / count for cls, count in class_counts.items()}\n",
    "    \n",
    "    # Create sample weights\n",
    "    sample_weights = [weights[int(label)] for label in y_train]\n",
    "    \n",
    "    return WeightedRandomSampler(\n",
    "        weights=sample_weights,\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d4363036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, \n",
    "                config: Config) -> nn.Module:\n",
    "    \"\"\"Training loop with early stopping and validation\"\"\"\n",
    "    \n",
    "    # Pre-calculate class weights more efficiently\n",
    "    y_train_all = []\n",
    "    print(\"Calculating class weights...\")\n",
    "    for i, (_, labels) in enumerate(train_loader):\n",
    "        y_train_all.extend(labels.numpy())\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Processed {i+1} batches...\")\n",
    "    \n",
    "    class_weights = compute_class_weight(\n",
    "        'balanced',\n",
    "        classes=np.unique(y_train_all),\n",
    "        y=y_train_all\n",
    "    )\n",
    "    class_weights = torch.FloatTensor(class_weights).to(config.DEVICE)\n",
    "    #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<befor edit\n",
    "    \n",
    "    \n",
    "#     class_weights = np.array(class_weights)\n",
    "\n",
    "#     adjustment = np.array([1.5, 1.2])\n",
    "#     class_weights = class_weights * adjustment  # ضرب numpy array في numpy array\n",
    "\n",
    "#     class_weights = torch.FloatTensor(class_weights).to(config.DEVICE)\n",
    "\n",
    "    \n",
    "    print(f\"Class weights: {class_weights}\")\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = FocalLoss(alpha=class_weights, gamma=2.0)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.LEARNING_RATE, weight_decay=0.04) #weight_decay=0.01\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.NUM_EPOCHS)\n",
    "    \n",
    "    # Training metrics\n",
    "    best_val_f1 = 0.0\n",
    "    patience = 5\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(config.NUM_EPOCHS):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        train_bar = tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{config.NUM_EPOCHS}\")\n",
    "        for batch_idx, (batch_X, batch_y) in enumerate(train_bar):\n",
    "            batch_X = batch_X.to(config.DEVICE)\n",
    "            batch_y = batch_y.to(config.DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "            train_labels.extend(batch_y.cpu().numpy())\n",
    "            \n",
    "            # Update progress bar\n",
    "            train_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_bar = tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}/{config.NUM_EPOCHS}\")\n",
    "            for batch_X, batch_y in val_bar:\n",
    "                batch_X = batch_X.to(config.DEVICE)\n",
    "                batch_y = batch_y.to(config.DEVICE)\n",
    "                \n",
    "                outputs = model(batch_X)\n",
    "                val_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "                val_labels.extend(batch_y.cpu().numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_f1 = f1_score(train_labels, train_preds, average='weighted')\n",
    "        val_f1 = f1_score(val_labels, val_preds, average='weighted')\n",
    "        val_balanced_acc = balanced_accuracy_score(val_labels, val_preds)\n",
    "        \n",
    "        # Per-class metrics for validation\n",
    "        val_f1_per_class = f1_score(val_labels, val_preds, average=None)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{config.NUM_EPOCHS} Results:\")\n",
    "        print(f\"  Train Loss: {train_loss/len(train_loader):.4f}, Train F1: {train_f1:.4f}\")\n",
    "        print(f\"  Val F1: {val_f1:.4f}, Val Balanced Acc: {val_balanced_acc:.4f}\")\n",
    "        print(f\"  Val F1 per class: [Class 0: {val_f1_per_class[0]:.3f}, Class 1: {val_f1_per_class[1]:.3f}]\")\n",
    "        \n",
    "        # Check class distribution in predictions\n",
    "        val_pred_dist = Counter(val_preds)\n",
    "        print(f\"  Val predictions distribution: {val_pred_dist}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), \"best_cardiac_model.pth\")\n",
    "            print(\"  New best model saved!\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"  Early stopping after {epoch+1} epochs\")\n",
    "                break\n",
    "        \n",
    "        scheduler.step()\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(\"best_cardiac_model.pth\"))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bf1dda6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model: nn.Module, test_loader: DataLoader, config: Config):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            batch_X = batch_X.to(config.DEVICE)\n",
    "            batch_y = batch_y.to(config.DEVICE)\n",
    "            \n",
    "            outputs = model(batch_X)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch_y.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    f1_weighted = f1_score(all_labels, all_preds, average='weighted')\n",
    "    f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
    "    balanced_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    print(\"\\n FINAL EVALUATION RESULTS:\")\n",
    "    print(f\"F1 Score (Weighted): {f1_weighted:.4f}\")\n",
    "    print(f\"F1 Score (Macro): {f1_macro:.4f}\")\n",
    "    print(f\"Balanced Accuracy: {balanced_acc:.4f}\")\n",
    "    \n",
    "    # Detailed classification report\n",
    "    print(\"\\n Classification Report:\")\n",
    "    target_names = ['Non-Critical (0)', 'Critical (1)']\n",
    "    print(classification_report(all_labels, all_preds, target_names=target_names, digits=4))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    print(\"\\n Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    return {\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'f1_macro': f1_macro,\n",
    "        'balanced_accuracy': balanced_acc,\n",
    "        'predictions': all_preds,\n",
    "        'labels': all_labels,\n",
    "        'probabilities': all_probs\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3ee7a0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting Cardiac Risk Prediction Model Training\n",
      "Device: cpu\n",
      "\n",
      " Loading and preprocessing data...\n",
      " Risk label distribution: {0: 5425, 1: 906, 2: 57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing patients: 6388it [00:49, 128.11it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total sequences created: 245506\n",
      "\n",
      " Balancing dataset...\n",
      "Original class distribution: Counter({0: 245026, 1: 480})\n",
      "Final class distribution: Counter({0: 8000, 1: 1440})\n",
      "\n",
      " Splitting data...\n",
      "Train: 6041, Val: 1511, Test: 1888\n",
      "Train distribution: Counter({0: 5119, 1: 922})\n",
      "Test distribution: Counter({0: 1600, 1: 288})\n",
      "\n",
      "🧠 Initializing model...\n",
      "Total parameters: 210,882\n",
      "\n",
      " Training model...\n",
      "Calculating class weights...\n",
      "Processed 1 batches...\n",
      "Processed 101 batches...\n",
      "Class weights: tensor([0.9766, 1.0246])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/15: 100%|██████████████████████████████████████████████| 189/189 [00:12<00:00, 15.44it/s, loss=0.1079]\n",
      "Validation Epoch 1/15: 100%|███████████████████████████████████████████████████████████| 48/48 [00:00<00:00, 81.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/15 Results:\n",
      "  Train Loss: 0.1540, Train F1: 0.6365\n",
      "  Val F1: 0.8345, Val Balanced Acc: 0.8602\n",
      "  Val F1 per class: [Class 0: 0.877, Class 1: 0.600]\n",
      "  Val predictions distribution: Counter({0: 1028, 1: 483})\n",
      "  New best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/15: 100%|██████████████████████████████████████████████| 189/189 [00:12<00:00, 15.51it/s, loss=0.0580]\n",
      "Validation Epoch 2/15: 100%|███████████████████████████████████████████████████████████| 48/48 [00:00<00:00, 83.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/15 Results:\n",
      "  Train Loss: 0.0913, Train F1: 0.8689\n",
      "  Val F1: 0.8578, Val Balanced Acc: 0.8994\n",
      "  Val F1 per class: [Class 0: 0.895, Class 1: 0.650]\n",
      "  Val predictions distribution: Counter({0: 1043, 1: 468})\n",
      "  New best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/15: 100%|██████████████████████████████████████████████| 189/189 [00:12<00:00, 14.87it/s, loss=0.0424]\n",
      "Validation Epoch 3/15: 100%|███████████████████████████████████████████████████████████| 48/48 [00:00<00:00, 72.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/15 Results:\n",
      "  Train Loss: 0.0723, Train F1: 0.9140\n",
      "  Val F1: 0.9110, Val Balanced Acc: 0.9319\n",
      "  Val F1 per class: [Class 0: 0.939, Class 1: 0.753]\n",
      "  Val predictions distribution: Counter({0: 1146, 1: 365})\n",
      "  New best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/15: 100%|██████████████████████████████████████████████| 189/189 [00:13<00:00, 14.27it/s, loss=0.0470]\n",
      "Validation Epoch 4/15: 100%|███████████████████████████████████████████████████████████| 48/48 [00:00<00:00, 73.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/15 Results:\n",
      "  Train Loss: 0.0647, Train F1: 0.9291\n",
      "  Val F1: 0.9457, Val Balanced Acc: 0.9589\n",
      "  Val F1 per class: [Class 0: 0.965, Class 1: 0.839]\n",
      "  Val predictions distribution: Counter({0: 1202, 1: 309})\n",
      "  New best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/15: 100%|██████████████████████████████████████████████| 189/189 [00:12<00:00, 14.86it/s, loss=0.0247]\n",
      "Validation Epoch 5/15: 100%|███████████████████████████████████████████████████████████| 48/48 [00:00<00:00, 56.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/15 Results:\n",
      "  Train Loss: 0.0592, Train F1: 0.9462\n",
      "  Val F1: 0.9554, Val Balanced Acc: 0.9687\n",
      "  Val F1 per class: [Class 0: 0.972, Class 1: 0.865]\n",
      "  Val predictions distribution: Counter({0: 1214, 1: 297})\n",
      "  New best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/15: 100%|██████████████████████████████████████████████| 189/189 [00:14<00:00, 13.18it/s, loss=0.0302]\n",
      "Validation Epoch 6/15: 100%|███████████████████████████████████████████████████████████| 48/48 [00:00<00:00, 58.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/15 Results:\n",
      "  Train Loss: 0.0438, Train F1: 0.9609\n",
      "  Val F1: 0.9366, Val Balanced Acc: 0.9598\n",
      "  Val F1 per class: [Class 0: 0.958, Class 1: 0.817]\n",
      "  Val predictions distribution: Counter({0: 1178, 1: 333})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/15: 100%|██████████████████████████████████████████████| 189/189 [00:14<00:00, 13.42it/s, loss=0.0516]\n",
      "Validation Epoch 7/15: 100%|███████████████████████████████████████████████████████████| 48/48 [00:00<00:00, 71.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/15 Results:\n",
      "  Train Loss: 0.0414, Train F1: 0.9621\n",
      "  Val F1: 0.9694, Val Balanced Acc: 0.9795\n",
      "  Val F1 per class: [Class 0: 0.981, Class 1: 0.905]\n",
      "  Val predictions distribution: Counter({0: 1235, 1: 276})\n",
      "  New best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/15: 100%|██████████████████████████████████████████████| 189/189 [00:11<00:00, 15.98it/s, loss=0.0173]\n",
      "Validation Epoch 8/15: 100%|███████████████████████████████████████████████████████████| 48/48 [00:00<00:00, 78.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/15 Results:\n",
      "  Train Loss: 0.0365, Train F1: 0.9694\n",
      "  Val F1: 0.9553, Val Balanced Acc: 0.9669\n",
      "  Val F1 per class: [Class 0: 0.972, Class 1: 0.865]\n",
      "  Val predictions distribution: Counter({0: 1216, 1: 295})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9/15: 100%|██████████████████████████████████████████████| 189/189 [00:12<00:00, 15.10it/s, loss=0.0103]\n",
      "Validation Epoch 9/15: 100%|███████████████████████████████████████████████████████████| 48/48 [00:00<00:00, 61.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/15 Results:\n",
      "  Train Loss: 0.0304, Train F1: 0.9750\n",
      "  Val F1: 0.9706, Val Balanced Acc: 0.9803\n",
      "  Val F1 per class: [Class 0: 0.982, Class 1: 0.909]\n",
      "  Val predictions distribution: Counter({0: 1237, 1: 274})\n",
      "  New best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10/15: 100%|█████████████████████████████████████████████| 189/189 [00:30<00:00,  6.12it/s, loss=0.0349]\n",
      "Validation Epoch 10/15: 100%|██████████████████████████████████████████████████████████| 48/48 [00:00<00:00, 74.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/15 Results:\n",
      "  Train Loss: 0.0240, Train F1: 0.9826\n",
      "  Val F1: 0.9790, Val Balanced Acc: 0.9697\n",
      "  Val F1 per class: [Class 0: 0.987, Class 1: 0.932]\n",
      "  Val predictions distribution: Counter({0: 1269, 1: 242})\n",
      "  New best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11/15: 100%|█████████████████████████████████████████████| 189/189 [00:23<00:00,  8.07it/s, loss=0.0242]\n",
      "Validation Epoch 11/15: 100%|██████████████████████████████████████████████████████████| 48/48 [00:01<00:00, 37.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/15 Results:\n",
      "  Train Loss: 0.0252, Train F1: 0.9826\n",
      "  Val F1: 0.9768, Val Balanced Acc: 0.9842\n",
      "  Val F1 per class: [Class 0: 0.986, Class 1: 0.927]\n",
      "  Val predictions distribution: Counter({0: 1247, 1: 264})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12/15: 100%|█████████████████████████████████████████████| 189/189 [00:17<00:00, 10.81it/s, loss=0.0130]\n",
      "Validation Epoch 12/15: 100%|██████████████████████████████████████████████████████████| 48/48 [00:00<00:00, 67.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/15 Results:\n",
      "  Train Loss: 0.0193, Train F1: 0.9892\n",
      "  Val F1: 0.9837, Val Balanced Acc: 0.9867\n",
      "  Val F1 per class: [Class 0: 0.990, Class 1: 0.948]\n",
      "  Val predictions distribution: Counter({0: 1260, 1: 251})\n",
      "  New best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13/15: 100%|█████████████████████████████████████████████| 189/189 [00:13<00:00, 13.74it/s, loss=0.0073]\n",
      "Validation Epoch 13/15: 100%|██████████████████████████████████████████████████████████| 48/48 [00:00<00:00, 60.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13/15 Results:\n",
      "  Train Loss: 0.0181, Train F1: 0.9916\n",
      "  Val F1: 0.9870, Val Balanced Acc: 0.9922\n",
      "  Val F1 per class: [Class 0: 0.992, Class 1: 0.958]\n",
      "  Val predictions distribution: Counter({0: 1261, 1: 250})\n",
      "  New best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14/15: 100%|█████████████████████████████████████████████| 189/189 [00:13<00:00, 13.90it/s, loss=0.0077]\n",
      "Validation Epoch 14/15: 100%|██████████████████████████████████████████████████████████| 48/48 [00:00<00:00, 78.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14/15 Results:\n",
      "  Train Loss: 0.0171, Train F1: 0.9922\n",
      "  Val F1: 0.9883, Val Balanced Acc: 0.9930\n",
      "  Val F1 per class: [Class 0: 0.993, Class 1: 0.962]\n",
      "  Val predictions distribution: Counter({0: 1263, 1: 248})\n",
      "  New best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15/15: 100%|█████████████████████████████████████████████| 189/189 [00:11<00:00, 15.88it/s, loss=0.0076]\n",
      "Validation Epoch 15/15: 100%|██████████████████████████████████████████████████████████| 48/48 [00:00<00:00, 72.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15/15 Results:\n",
      "  Train Loss: 0.0179, Train F1: 0.9929\n",
      "  Val F1: 0.9883, Val Balanced Acc: 0.9930\n",
      "  Val F1 per class: [Class 0: 0.993, Class 1: 0.962]\n",
      "  Val predictions distribution: Counter({0: 1263, 1: 248})\n",
      "\n",
      " Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████| 59/59 [00:00<00:00, 72.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " FINAL EVALUATION RESULTS:\n",
      "F1 Score (Weighted): 0.9824\n",
      "F1 Score (Macro): 0.9667\n",
      "Balanced Accuracy: 0.9880\n",
      "\n",
      " Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Non-Critical (0)     0.9994    0.9794    0.9893      1600\n",
      "    Critical (1)     0.8969    0.9965    0.9441       288\n",
      "\n",
      "        accuracy                         0.9820      1888\n",
      "       macro avg     0.9481    0.9880    0.9667      1888\n",
      "    weighted avg     0.9837    0.9820    0.9824      1888\n",
      "\n",
      "\n",
      " Confusion Matrix:\n",
      "[[1567   33]\n",
      " [   1  287]]\n",
      "\n",
      " Model saved to D:\\Projects\\Cardiac Patient Monitoring System\\models\\lstm\\model.pth\n",
      " Scaler saved to D:\\Projects\\Cardiac Patient Monitoring System\\models\\lstm\\scaler.pkl\n",
      "\n",
      " Training completed successfully!\n",
      "Best F1 Score: 0.9824\n",
      "Best Balanced Accuracy: 0.9880\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    config = Config()\n",
    "    \n",
    "    print(\" Starting Cardiac Risk Prediction Model Training\")\n",
    "    print(f\"Device: {config.DEVICE}\")\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    print(\"\\n Loading and preprocessing data...\")\n",
    "    X, y, scaler = load_and_preprocess_data(config)\n",
    "    \n",
    "    # Balance dataset\n",
    "    print(\"\\n Balancing dataset...\")\n",
    "    X_balanced, y_balanced = balance_dataset(X, y)\n",
    "    \n",
    "    # Split data\n",
    "    print(\"\\n Splitting data...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_balanced, y_balanced, \n",
    "        test_size=0.2, \n",
    "        stratify=y_balanced, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train, \n",
    "        test_size=0.2, \n",
    "        stratify=y_train, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
    "    print(f\"Train distribution: {Counter(y_train)}\")\n",
    "    print(f\"Test distribution: {Counter(y_test)}\")\n",
    "    \n",
    "    # Create datasets and data loaders\n",
    "    train_dataset = PatientDataset(torch.FloatTensor(X_train), torch.LongTensor(y_train))\n",
    "    val_dataset = PatientDataset(torch.FloatTensor(X_val), torch.LongTensor(y_val))\n",
    "    test_dataset = PatientDataset(torch.FloatTensor(X_test), torch.LongTensor(y_test))\n",
    "    \n",
    "    # Create weighted sampler for training\n",
    "    weighted_sampler = create_weighted_sampler(y_train)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=config.BATCH_SIZE, \n",
    "        sampler=weighted_sampler,\n",
    "        pin_memory=False,  # Disable for CPU\n",
    "        num_workers=0  # Fix Windows multiprocessing issue\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=config.BATCH_SIZE, \n",
    "        shuffle=False,\n",
    "        pin_memory=False,  # Disable for CPU\n",
    "        num_workers=0  # Fix Windows multiprocessing issue\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=config.BATCH_SIZE, \n",
    "        shuffle=False,\n",
    "        pin_memory=False,  # Disable for CPU\n",
    "        num_workers=0  # Fix Windows multiprocessing issue\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    print(\"\\n🧠 Initializing model...\")\n",
    "    model = ImprovedLSTMClassifier(\n",
    "        input_size=len(config.SIGNAL_NAMES),\n",
    "        hidden_size=config.HIDDEN_SIZE,\n",
    "        num_classes=2,\n",
    "        dropout_rate=config.DROPOUT_RATE\n",
    "    ).to(config.DEVICE)\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\n Training model...\")\n",
    "    model = train_model(model, train_loader, val_loader, config)\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(\"\\n Evaluating model...\")\n",
    "    results = evaluate_model(model, test_loader, config)\n",
    "    \n",
    "    import joblib\n",
    "    \n",
    "    os.makedirs(r\"D:\\Projects\\Cardiac Patient Monitoring System\\models\\lstm\", exist_ok=True)\n",
    "    MODEL_PATH = r\"D:\\Projects\\Cardiac Patient Monitoring System\\models\\lstm\\model.pth\"\n",
    "    SCALER_PATH = r\"D:\\Projects\\Cardiac Patient Monitoring System\\models\\lstm\\scaler.pkl\"\n",
    "\n",
    "    torch.save(model.state_dict(), MODEL_PATH)\n",
    "    joblib.dump(scaler, SCALER_PATH)\n",
    "    print(f\"\\n Model saved to {MODEL_PATH}\")\n",
    "    print(f\" Scaler saved to {SCALER_PATH}\")\n",
    "\n",
    "    # Save results\n",
    "    with open('cardiac_model_results.pkl', 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "    \n",
    "    print(\"\\n Training completed successfully!\")\n",
    "    print(f\"Best F1 Score: {results['f1_weighted']:.4f}\")\n",
    "    print(f\"Best Balanced Accuracy: {results['balanced_accuracy']:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f24b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3cab51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ede2cb3c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523197c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ecg_env)",
   "language": "python",
   "name": "your_env_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
